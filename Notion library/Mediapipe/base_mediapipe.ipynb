{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb175bf5",
   "metadata": {},
   "source": [
    "# Mediapipe\n",
    "\n",
    "### la bibliothèque python mediapipe pour détecter les repères du visage et de la main. Nous utiliserons un modèle holistique des solutions mediapipe pour détecter tous les repères du visage et de la main. Nous verrons également comment accéder à différents repères du visage et des mains qui peuvent être utilisés pour différentes applications de vision par ordinateur telles que la détection du langage des signes, la détection de la somnolence, etc.\n",
    "\n",
    "### Bibliothèques requises\n",
    "\n",
    "   * Mediapipe est une bibliothèque multiplateforme développée par Google qui fournit d'étonnantes solutions ML prêtes à l'emploi pour les tâches de vision par ordinateur.\n",
    "   * La bibliothèque OpenCV en python est une bibliothèque de vision par ordinateur largement utilisée pour l'analyse d'images, le traitement d'images, la détection, la reconnaissance, etc.\n",
    "   \n",
    "### Installation des bibliothèques requises\n",
    "\n",
    "     pip installer opencv-python mediapipe\n",
    "     \n",
    "### Vous trouverez ci-dessous l'approche par étapes pour la détection des points de repère du visage et de la main\n",
    "\n",
    "## ÉTAPE-1 : Importez toutes les bibliothèques nécessaires. Dans notre cas, seules deux bibliothèques sont nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafb7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5016892",
   "metadata": {},
   "source": [
    "## ÉTAPE-2 : Initialisation du modèle holistique et des utilitaires de dessin pour détecter et dessiner des points de repère sur l'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4187ff67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Grabbing the Holistic Model from Mediapipe and\n",
    "# Initializing the Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    " \n",
    "# Initializing the drawing utils for drawing the facial landmarks on image\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a38b2",
   "metadata": {},
   "source": [
    "### Examinons les paramètres du modèle holistique :\n",
    "       Holistique(\n",
    "\n",
    "      static_image_mode=Faux,\n",
    "\n",
    "      model_complexity=1,\n",
    "\n",
    "      smooth_landmarks=Vrai,\n",
    "\n",
    "      min_detection_confidence=0.5,\n",
    "\n",
    "      min_tracking_confidence=0.5\n",
    "\n",
    "        )\n",
    "\n",
    "    * static_image_mode : Il est utilisé pour spécifier si les images d'entrée doivent être traitées comme des images statiques ou comme un flux vidéo. La valeur par défaut est Faux.\n",
    "    \n",
    "    * model_complexity : Il est utilisé pour spécifier la complexité du modèle de point de repère de pose : 0, 1 ou 2. À mesure que la complexité du modèle du modèle augmente, la précision du point de repère et la latence augmentent. La valeur par défaut est 1.\n",
    "    \n",
    "    * smooth_landmarks : ce paramètre est utilisé pour réduire la gigue dans la prédiction en filtrant les repères de pose sur différentes images d'entrée. La valeur par défaut est Vrai.\n",
    "    * min_detection_confidence : il est utilisé pour spécifier la valeur de confiance minimale avec laquelle la détection à partir du modèle de détection de personne doit être considérée comme réussie. Peut spécifier une valeur dans [0.0,1.0]. La valeur par défaut est 0,5.\n",
    "    * min_tracking_confidence : il est utilisé pour spécifier la valeur de confiance minimale avec laquelle la détection à partir du modèle de suivi de points de repère doit être considérée comme réussie. Peut spécifier une valeur dans [0.0,1.0]. La valeur par défaut est 0,5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2929c1",
   "metadata": {},
   "source": [
    "## ÉTAPE-3 : détection des points de repère du visage et de la main à partir de l'image. Le modèle holistique traite l'image et produit des points de repère pour le visage, la main gauche, la main droite et détecte également la pose du \n",
    "\n",
    "    1. Capturez les images en continu à partir de la caméra à l'aide d'OpenCV.\n",
    "    2. Convertissez l'image BGR en image RVB et faites des prédictions à l'aide d'un modèle holistique initialisé.\n",
    "    3. Les prédictions faites par le modèle holistique sont enregistrées dans la variable de résultats à partir de laquelle nous pouvons accéder aux points de repère en utilisant respectivement results.face_landmarks, results.right_hand_landmarks, results.left_hand_landmarks.\n",
    "    4.Dessinez les points de repère détectés sur l'image à l'aide de la fonction draw_landmarks des utilitaires de dessin.\n",
    "    5.Affichez l'image résultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e3214b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0) in VideoCapture is used to connect to your computer's default camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing current time and precious time for calculating the FPS\n",
    "previousTime = 0\n",
    "currentTime = 0\n",
    "\n",
    "while capture.isOpened():\n",
    "    # capture frame by frame\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # resizing the frame for better view\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "    # Converting the from BGR to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Making predictions using holistic model\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    # Converting back the RGB image to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Drawing the Facial Landmarks\n",
    "    mp_drawing.draw_landmarks(\n",
    "    image,\n",
    "    results.face_landmarks,\n",
    "    mp_holistic.FACEMESH_CONTOURS,\n",
    "    mp_drawing.DrawingSpec(\n",
    "        color=(255,0,255),\n",
    "        thickness=1,\n",
    "        circle_radius=1\n",
    "    ),\n",
    "    mp_drawing.DrawingSpec(\n",
    "        color=(0,255,255),\n",
    "        thickness=1,\n",
    "        circle_radius=1\n",
    "    )\n",
    "    )\n",
    "\n",
    "    # Drawing Right hand Land Marks\n",
    "    mp_drawing.draw_landmarks(\n",
    "    image,\n",
    "    results.right_hand_landmarks,\n",
    "    mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "\n",
    "    # Drawing Left hand Land Marks\n",
    "    mp_drawing.draw_landmarks(\n",
    "    image,\n",
    "    results.left_hand_landmarks,\n",
    "    mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "    \n",
    "    # Calculating the FPS\n",
    "    currentTime = time.time()\n",
    "    fps = 1 / (currentTime-previousTime)\n",
    "    previousTime = currentTime\n",
    "\n",
    "    # Displaying FPS on the image\n",
    "    cv2.putText(image, str(int(fps))+\" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "    # Enter key 'q' to break the loop\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When all the process is done\n",
    "# Release the capture and destroy all windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54818d54",
   "metadata": {},
   "source": [
    "### Le modèle holistique produit 468 repères de visage, 21 repères de gauche et 21 repères de droite. Les points de repère individuels sont accessibles en spécifiant l'index du point de repère requis. Exemple : résultats.left_hand_landmarks.landmark[0]. Vous pouvez obtenir l'index de tous les points de repère individuels en utilisant le code ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f6fce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandLandmark.WRIST 0\n",
      "HandLandmark.THUMB_CMC 1\n",
      "HandLandmark.THUMB_MCP 2\n",
      "HandLandmark.THUMB_IP 3\n",
      "HandLandmark.THUMB_TIP 4\n",
      "HandLandmark.INDEX_FINGER_MCP 5\n",
      "HandLandmark.INDEX_FINGER_PIP 6\n",
      "HandLandmark.INDEX_FINGER_DIP 7\n",
      "HandLandmark.INDEX_FINGER_TIP 8\n",
      "HandLandmark.MIDDLE_FINGER_MCP 9\n",
      "HandLandmark.MIDDLE_FINGER_PIP 10\n",
      "HandLandmark.MIDDLE_FINGER_DIP 11\n",
      "HandLandmark.MIDDLE_FINGER_TIP 12\n",
      "HandLandmark.RING_FINGER_MCP 13\n",
      "HandLandmark.RING_FINGER_PIP 14\n",
      "HandLandmark.RING_FINGER_DIP 15\n",
      "HandLandmark.RING_FINGER_TIP 16\n",
      "HandLandmark.PINKY_MCP 17\n",
      "HandLandmark.PINKY_PIP 18\n",
      "HandLandmark.PINKY_DIP 19\n",
      "HandLandmark.PINKY_TIP 20\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Code to access landmarks\n",
    "for landmark in mp_holistic.HandLandmark:\n",
    "    print(landmark, landmark.value)\n",
    " \n",
    "print(mp_holistic.HandLandmark.WRIST.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd6dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d02d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
